{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "from math import floor\n",
    "\n",
    "\n",
    "def minmax_norm_numpy(data):\n",
    "    # Function for min-max normalizing a vector/array\n",
    "    # Calculate the minimum and maximum values in the array\n",
    "    min_val = data.min()\n",
    "    max_val = data.max()\n",
    "\n",
    "    # Perform min-max normalization\n",
    "    normalized_data = (data - min_val) / (max_val - min_val)\n",
    "    return normalized_data\n",
    "\n",
    "def stand_numpy(data):\n",
    "    # Function for standardizing a vector/array based on mean and standard deviation\n",
    "    # Calculate the mean and standard deviation of the array\n",
    "    mean_val = data.mean()\n",
    "    std_dev = data.std()\n",
    "\n",
    "    # Perform standardization\n",
    "    standardized_data = (data - mean_val) / std_dev\n",
    "    return standardized_data\n",
    "\n",
    "def getMatchInds(ft_qry, ft_ref, metric='cosine'):\n",
    "    # Function for performing VPR matching\n",
    "    # metric: 'euclidean' or 'cosine'\n",
    "\n",
    "    dMat = cdist(ft_ref,ft_qry,metric)\n",
    "    mInds = np.argsort(dMat,axis=0)[:5]\n",
    "    return dMat, mInds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################ This cell sets up some data variables #############################\n",
    "dataset = 'Dataset_Name'\n",
    "\n",
    "# Load VPR query and reference descriptors/features\n",
    "query_feats = np.load('Path to Query Features')\n",
    "ref_feats = np.load('Path to Reference Features')\n",
    "\n",
    "# Load ground truth information\n",
    "# This should have a shape of M x 2, where:\n",
    "# M = number of query images;\n",
    "# The first column contains the query index;\n",
    "# The second column contains a list for each query of the reference image indices\n",
    "#  which correspond to matching reference images\n",
    "gt = np.load('Path to Ground Truth Info', allow_pickle=True)\n",
    "\n",
    "# Store just the column containing matching reference indices for each query\n",
    "gt_match_idxs = gt[:,1]\n",
    "\n",
    "# Scale factor between query and reference if there is a difference in sampling\n",
    "ref_scaler = 1\n",
    "# Size of the coarse position prior (in number of images)\n",
    "sect_len = 75\n",
    "# Step size when iterating through the dataset\n",
    "step_scaler = 1\n",
    "\n",
    "diff_type = 'Absolute'\n",
    "\n",
    "# List of the sequence lengths which are being tested\n",
    "sequence_list = [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "# Extra list to include case of single query\n",
    "sequence_list_labels = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# This cell computes the baseline VPR performance and appearance variation vectors ###########################\n",
    "print('Calculating baselines')\n",
    "\n",
    "# Compute the base VPR distance matrix and proposed matches\n",
    "dMat, mInds = getMatchInds(query_feats, ref_feats, metric='cosine')\n",
    "\n",
    "\n",
    "section_mInds = np.zeros((query_feats.shape[0],)).astype(int)\n",
    "section_all_mInds = np.zeros((query_feats.shape[0],10)).astype(int)\n",
    "\n",
    "coarse_prior_appearance_var = np.zeros((floor(query_feats.shape[0]/sect_len)*step_scaler, query_feats.shape[1]))\n",
    "# Create a variable for tracking the brute for check of recall per chunk per sequence length\n",
    "section_seq_avg_recall = np.zeros((len(sequence_list)+1, floor(query_feats.shape[0]/sect_len)*step_scaler))\n",
    "\n",
    "# Iterating through the dataset using different center images for the coarse position prior\n",
    "for k in tqdm(range(0,query_feats.shape[0]-sect_len,int(sect_len/step_scaler))):\n",
    "\n",
    "    # Retrieve the distance matrix, ground truth match indices and VPR match indices\n",
    "    #  for the particular section/chunk of the dataset\n",
    "    sect_dMat = dMat[k*ref_scaler:(k+sect_len)*ref_scaler, k:k+sect_len]\n",
    "    sect_gt = gt_match_idxs[k:k+sect_len]\n",
    "    sect_mInds = np.argsort(sect_dMat,axis=0)[:10]\n",
    "\n",
    "    \n",
    "    # Adjust the match indices to be with respect to the entire\n",
    "    #  reference database instead of the particular chunk\n",
    "    sect_mInds = sect_mInds+(k*ref_scaler)\n",
    "    section_mInds[k:k+sect_len] = sect_mInds[0,:]\n",
    "    section_all_mInds[k:k+sect_len,:] = sect_mInds.T\n",
    "\n",
    "    # Retrieve reference features for places within the coarse position prior\n",
    "    section_ref_feats = ref_feats[k*ref_scaler:(k+sect_len)*ref_scaler]\n",
    "    # Add new axis for vectorising computation of mean differences\n",
    "    reshaped_ref_feats = section_ref_feats[:,np.newaxis,:]\n",
    "    # Compute the feature-wise mean difference between reference places in the coarse position prior\n",
    "    sect_ref_feats_diff = np.mean(reshaped_ref_feats - section_ref_feats, axis=1)\n",
    "\n",
    "    coarse_prior_appearance_var[int((k/sect_len)*step_scaler), :] = np.std(sect_ref_feats_diff, axis=0)\n",
    "\n",
    "    ############## Evaluate the recall the coarse position prior area ###########################\n",
    "\n",
    "    rec_count = 0\n",
    "    match_idx = 0\n",
    "    for j in range(sect_len):\n",
    "        if sect_mInds[0,j] in sect_gt[j]:\n",
    "            rec_count += 1\n",
    "    section_seq_avg_recall[0,int((k/sect_len)*step_scaler)] = rec_count/sect_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################### This cell computes VPR performance for different sequence lengths #############################\n",
    "print('Calculating sequences')\n",
    "print('Starting VPR')\n",
    "\n",
    "for idx, n in enumerate(sequence_list):\n",
    "\n",
    "    # Load the distance matrix for VPR using the sequence length n\n",
    "    dMat = np.load('Path to sequence-based distance matrix for seq len n'.format(n))\n",
    "    mInds = np.argsort(dMat,axis=0)[:5]\n",
    "\n",
    "    for k in tqdm(range(0,query_feats.shape[0]-sect_len,int(sect_len/step_scaler))):\n",
    "\n",
    "        # Retrieve the distance matrix, ground truth match indices and VPR match indices\n",
    "        #  for the particular sequence length and section/chunk of the dataset\n",
    "        sect_dMat = dMat[k*ref_scaler:(k+sect_len)*ref_scaler, k:k+sect_len]\n",
    "        sect_gt = gt_match_idxs[k:k+sect_len]\n",
    "        sect_mInds = np.argsort(sect_dMat,axis=0)[:5]\n",
    "\n",
    "        # Adjust the match indices to be with respect to the entire\n",
    "        #  reference database instead of the particular chunk\n",
    "        sect_mInds = sect_mInds+(k*ref_scaler)\n",
    "\n",
    "        rec_count = 0\n",
    "        for j in range(sect_len):\n",
    "            if sect_mInds[0,j] in sect_gt[j]:\n",
    "                rec_count += 1\n",
    "        section_seq_avg_recall[idx+1,int((k/sect_len)*step_scaler)] = rec_count/sect_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# This cell computes the min sequence length for the desired recall in each dataset chunk ##################\n",
    "# Setup a variable for storing the minimum sequence length values\n",
    "min_seq_len_for_target = np.zeros((floor(query_feats.shape[0]/sect_len)*step_scaler,))\n",
    "# This is the nominated minimum recall performance target\n",
    "target_recall_performance = 0.75\n",
    "\n",
    "# Here we plot the recall in each dataset chunk for increasing sequence lengths\n",
    "#  This is useful for tuning the desired recall performance\n",
    "f, ax = plt.subplots()\n",
    "for n in range(0,floor(query_feats.shape[0]/sect_len)*step_scaler):\n",
    "    ax.plot(np.arange(1,22,2).astype(int), section_seq_avg_recall[:, n]*100, label='{}'.format(n))\n",
    "plt.legend()\n",
    "plt.axhline(target_recall_performance*100, color='k', linestyle='--')\n",
    "plt.xlabel('Sequence Length', fontdict={\"size\":13})\n",
    "plt.ylabel('Recall @ 1 (\\%)', fontdict={\"size\":13})\n",
    "plt.ylim(0, 100)\n",
    "plt.title('Sequence Length Sweep'.format(dataset))\n",
    "plt.xticks(sequence_list_labels)\n",
    "\n",
    "# Plot the min sequence length for each dataset chunk which exceeds the target performance\n",
    "#  or the one which maximises recall in the case it does not achieve the target\n",
    "f, ax = plt.subplots()\n",
    "f.set_size_inches(4.5, 3.5)\n",
    "for n in range(floor(query_feats.shape[0]/sect_len)*step_scaler):\n",
    "    vals = section_seq_avg_recall[:, n]-target_recall_performance # abs()\n",
    "    if np.all(vals < 0):\n",
    "        vals = abs(vals)\n",
    "    else:\n",
    "        vals[vals < 0] = 1\n",
    "    min_seq_len_for_target[n] = sequence_list_labels[vals.argmin()]\n",
    "    ax.scatter(sequence_list_labels[vals.argmin()], section_seq_avg_recall[vals.argmin(), n]*100)\n",
    "plt.axhline(target_recall_performance*100, color='k', linestyle='--')\n",
    "plt.xlabel('Sequence Length', fontdict={\"size\":13})\n",
    "plt.ylabel('Recall @ 1 (\\%)', fontdict={\"size\":13})\n",
    "ax.set_yticks([])\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(sequence_list_labels)\n",
    "plt.title(r'Sequence Lengths for Target Recall ($s_{c}$)')\n",
    "\n",
    "########################## Here is where you would save data for use in the multivariate-NN-regression #################\n",
    "########################## Variables: min_seq_len_for_target, and coarse_prior_appearance_var          ##################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ad4369d77524a453458575f2db98d7a70a567551801a00da99bdb6a5bad7837"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
